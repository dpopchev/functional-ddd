## Gravitational theory testing is a data science challenge

::: columns

:::: column

### Gravitational wave detection

![Detected wave signal](data/images/gravitatioanl-wave-detection.png){width=100%}

- Data processing took several months.
- Signal lasted $0.2$ seconds
- Two BHs $\sim 30 \times M_{sun}$ merging $d \sim 1.3\times10^9$ ly
- $5.1 \sigma$ or $99.99997\%$ confidence

::::

:::: column

### Black hole image

![Radio composite image](data/images/blackhole-image.png){width=55%}

- $10$ day observation
- $2$ years data processing

::::

:::

### Bottom line:

General relativity remains one of the best models made by human kind!

## Universe is expanding faster, than expected...

::: columns

:::: {.column width=40%}

### Open questions

- What is dark matter?
- What is dark energy?
- How to quantize gravity?

::::

:::: {.column width=60%}

### Growing discrepancy of Hubble constant value

![Hubble constant measured value per method timeline](data/images/cosmology-crisis.png){width=75%}

::::

:::

## ...and the computational demand grows too.

### Why it matters:

- Different data sources -- different format.
- New models demand recalculation.
- More often the software solutions are fragile.

### Bottom line:

Modern problems require modern solutions to allow resilience and scalability.
